# Step 1 — Baseline Vulnerability Measurement

## Status: Complete (external / pre-project)

## Purpose
Measure how vulnerable the target LLM is to prompt injection attacks **before** any defenses are applied.

## What This Step Does
- Sends a battery of known jailbreak prompts directly to the LLM (no filtering)
- Records the raw response for each attack
- Calculates a **baseline attack success rate (ASR)**
- This ASR becomes the benchmark that Steps 2–6 must improve upon

## Dataset Used
- **ReNeLLM-Jailbreak** (`data/renellm/renellm_jailbreak.json`)
  - 500+ jailbreak prompts generated by the ReNeLLM method on AdvBench
  - Each record has: `nested_prompt`, `claude2_output`, `original_harm_behavior`, `model_label`
  - Paper: [A Wolf in Sheep's Clothing (arXiv:2311.08268)](https://arxiv.org/abs/2311.08268)

## Key Findings (from dataset)
- Attack techniques observed: LaTeX table manipulation, language mixing (Chinese + English + typos),
  nested instructions in benign wrappers, step-by-step harmful action breakdowns
- Baseline ASR on Claude 2: ~high (dataset only contains successful jailbreaks)

## Output
- Baseline ASR metric → feeds into Step 6 final comparison
- No code artifacts (measurement was done externally using the ReNeLLM dataset)

## Next Step
→ **Step 2: Prefilter** — intercept suspicious inputs before they reach the LLM
